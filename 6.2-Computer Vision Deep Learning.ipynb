{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are some examples of how we might use neural networks for robotics\n",
    "\n",
    "Check out how the SFU Autonomy Lab uses neural networks (a specific kind, called a deep neural network) to detect hands and faces with super high accuracy!\n",
    "\n",
    "https://www.youtube.com/watch?v=7vbmOWVxGPU&list=PLF9FF3765A9C8A57F&index=17\n",
    "\n",
    "The hand/face detector was trained with thousands of pictures of hands and faces. It is so accurate that it can be used to control a flying drone!\n",
    "\n",
    "\n",
    "## Imagine a \"seeing eye robot\" that can read out signs\n",
    "\n",
    "Imagine that we want to create a \"seeing eye robot\" that can read out signs. In other words, it takes a photo and converts the pixels into coherent, readable symbols.\n",
    "\n",
    "## Hand-writing recognition\n",
    "\n",
    "Using a simple handwritten digit dataset, we'll see how a neural network can be used to recognize hand-written numbers from pictures. \n",
    "\n",
    "This is the same technology that is used to recognize your postal code hand-written on letters on envelopes. \n",
    "\n",
    "# The MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n",
       "       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n",
       "       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n",
       "       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n",
       "       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n",
       "       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n",
       "       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "digits.images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess what number is represented in the \"image\" above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(digits.images[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the MNIST dataset?\n",
    "\n",
    "In this dataset, 1797 hand-written digits are collected and labeled with their actual values (between 1 to 10), as shown below. [Source](http://corochann.com/mnist-dataset-introduction-1138.html). \n",
    "\n",
    "We can train a neural network so that if our program is shown a new handwritten digit, it can identify its value.\n",
    "\n",
    "![caption](img/mnist_dataset.png)\n",
    "\n",
    "This exercise was originally used by [creAIte](http://aiandart.wixsite.com/creaite)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting MNIST into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "pixels = mnist.data # The input values are the pixelated data\n",
    "labels = mnist.target # The mnist targets are the correct labels\n",
    "\n",
    "# Pixels train + labels train are used to train the classifier\n",
    "# Pixels test + labels test are used to test\n",
    "\n",
    "pixels_train, pixels_test, labels_train, labels_test = train_test_split(pixels, labels, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How big are our training and test sets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "print(len(labels_train))\n",
    "print(len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forget feature extraction, let's just get a neural network and train it!\n",
    "\n",
    "Neural networks are getting popular as way for a program to do end-to-end learning. Instead of having a feature extraction step, you can input raw data into the neural network, tell the classifier what it represents, and have it learn the features/associations by itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=300, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier # MLP: Multi-layer Perceptron\n",
    "\n",
    "neural_network = MLPClassifier(max_iter = 300, learning_rate_init=0.001)\n",
    "neural_network.fit(pixels_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How accurate is our neural network at recognizing hand-written numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Run our images through our neural network\n",
    "labels_test_result = neural_network.predict(pixels_test)\n",
    "\n",
    "# Compare our outputted labels with the actual labels\n",
    "accuracy = accuracy_score(labels_test_result, labels_test)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does changing the learning rate affect the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try changing the parameters and see what happens to the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that our classifier is trained, let's show it one picture to see how well it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the result look like the number we saw before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained neural network on a given image (the one we used at the very beginning)\n",
    "result = neural_network.predict(mnist.data[0].reshape(1, -1))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "\n",
    "Deep neural networks (also known as convolutional neural networks) have many layers. That's why they're called \"deep!\" You can see how adding more layers can improve the ability of the classifier to separate clusters more effectively. However, adding too many layers can cause overfitting.\n",
    "\n",
    "## Try this\n",
    "\n",
    "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.98832&showTestData=false&discretize=true&percTrainData=30&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&stepButton_hide=true&activation_hide=true&noise_hide=true&regularization_hide=true&batchSize_hide=true&learningRate_hide=true&regularizationRate_hide=true&problem_hide=true&percTrainData_hide=true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Ethics Reflections\n",
    "\n",
    "- Who made the labels? \n",
    "- Neural networks are complicated, can we visualize them to understand how they work (like clustering?)\n",
    "- What do you think it means for a classifier to be \"black box\"?\n",
    "- What are the implications if the neural network performs the feature extraction step instead of humans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
